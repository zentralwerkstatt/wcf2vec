{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "\n",
    "https://github.com/chrisjmccormick/word2vec_commented\n",
    "\n",
    "http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/\n",
    "\n",
    "https://github.com/cbellei/word2veclite\n",
    "\n",
    "https://keras.io/preprocessing/sequence/#skipgrams\n",
    "\n",
    "https://github.com/Hironsan/awesome-embedding-models/blob/master/examples/skip-gram_with_ns.py\n",
    "\n",
    "https://github.com/nzw0301/keras-examples\n",
    "\n",
    "https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import gensim\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: implement loading of pickled wcf matrix\n",
    "# Load from pickle file\n",
    "# with open('data.pic', 'rb') as f:\n",
    "#     source = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "corpus = 'c/text8.txt'\n",
    "dim = 300 # word2vec default: 300\n",
    "window_size = 5 # word2vec default: 5\n",
    "# min_count = 5 # word2vec default: 5 - set to 0 to disable\n",
    "max_vocab = 10000\n",
    "sampling_factor = 1e-03 # word2vec default: 1e-03 - set to 0 to disable\n",
    "positive_samples = 10000 # gensim.word2vec.Text8Corpus default: 10000 words/sequence -> Keras skipgram function\n",
    "negative_samples = 5 # word2vec default: 5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text size: 100000000 characters\n",
      "Converting text to word list...\n"
     ]
    }
   ],
   "source": [
    "# Read corpus\n",
    "with open(corpus, 'r') as f: text = f.read()\n",
    "\n",
    "# Make it all lower case    \n",
    "text = text.lower()\n",
    "\n",
    "# Remove all punctuation\n",
    "exclude = set(string.punctuation)\n",
    "text = ''.join(char for char in text if char not in exclude)\n",
    "\n",
    "print('Text size: '+ str(len(text)) + ' characters')\n",
    "\n",
    "# Truncate to a million characters for quick testing\n",
    "text = text\n",
    "\n",
    "# Make list\n",
    "print('Converting text to word list...')\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum frequency: 126, word: recognise\n",
      "Vocabulary size: 10000 words\n"
     ]
    }
   ],
   "source": [
    "# Get frequencies\n",
    "frequencies = Counter(text).most_common()\n",
    "\n",
    "# Eliminate words that appear less than min_count times\n",
    "# frequencies = OrderedDict({word: i for word, i in frequencies if i >= min_count})\n",
    "\n",
    "# Keep the vocabulary to max_vocab\n",
    "frequencies = OrderedDict(frequencies[:max_vocab])\n",
    "min_frequency = min(frequencies, key=frequencies.get)\n",
    "print('Minimum frequency: ' + str(frequencies[min_frequency]) + ', word: ' + str(min_frequency))\n",
    "\n",
    "# Map words to integers\n",
    "token2id = {word: i for i, word in enumerate(frequencies.keys())}\n",
    "id2token = {i: word for i, word in enumerate(frequencies.keys())}\n",
    "\n",
    "# Length of dictionary is size of vocabulary\n",
    "vocabulary_size = len(token2id)\n",
    "\n",
    "# Encode\n",
    "text = [token2id[token] for token in text if token in token2id]\n",
    "\n",
    "# Make sampling table for subsampling (starts at 1!)\n",
    "sampling_table = make_sampling_table(size=vocabulary_size+1, sampling_factor=sampling_factor)\n",
    "\n",
    "print('Vocabulary size: ' + str(vocabulary_size) + ' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling WCF matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc1c4a549174168b5ca5335ff09f17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15268026), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples in matrix: 152680230 samples\n",
      "Creating sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "# Create square word-context-frequency matrix\n",
    "# TO DO: make this faster with multiprocessing\n",
    "print('Filling WCF matrix...')\n",
    "mat = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "for i, wi in (enumerate(tqdm_notebook(text))):\n",
    "    window_start = max(0, i - window_size)\n",
    "    window_end = min(len(text), i + window_size + 1)\n",
    "    for j in range(window_start, window_end):\n",
    "        if j != i:\n",
    "            wj = text[j]\n",
    "            mat[wi][wj]+=1            \n",
    "print('Total samples in matrix: ' + str(mat.sum()) + ' samples')\n",
    "        \n",
    "# Create sparse LIL representation of square word-context matrix\n",
    "print('Creating sparse matrix...')\n",
    "coo_mat = coo_matrix(mat)\n",
    "lil_mat = coo_mat.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 300)       3000000     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 300)       3000000     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1, 1)         0           embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1)            0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1)            0           reshape_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,000,000\n",
      "Trainable params: 6,000,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "\n",
    "t_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "t = Embedding(vocabulary_size, dim)(t_inputs)\n",
    "c_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "c  = Embedding(vocabulary_size, dim)(c_inputs)\n",
    "o = Dot(axes=2)([t, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "sgns = Model(inputs=[t_inputs, c_inputs], outputs=o)\n",
    "sgns.summary()\n",
    "sgns.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, testword):\n",
    "        self.testword = testword\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.most_similar(positive=[self.testword], negative=[], topn=3)\n",
    "        \n",
    "    def most_similar(self, positive, negative=[], topn=10):\n",
    "        \n",
    "        # Gensim word vector file format\n",
    "        with open('vectors.txt' ,'w') as f:\n",
    "            f.write('{} {}\\n'.format(vocabulary_size, dim))\n",
    "            vectors = sgns.get_weights()[0]\n",
    "            for token, i in token2id.items():\n",
    "                f.write('{} {}\\n'.format(token, ' '.join(map(str, list(vectors[i,:])))))\n",
    "                \n",
    "        # Load and get most similar vectors\n",
    "        w2v = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)\n",
    "        out = w2v.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        print(positive, negative, out)\n",
    "        \n",
    "        \n",
    "class WordContextMatrixDataset(Sequence):\n",
    "\n",
    "    def __init__(self, lil_mat, positive_samples=10000, negative_samples=5):\n",
    "\n",
    "        self.lil_mat = lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(lil_mat)\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "        self.batch_size = positive_samples + negative_samples\n",
    "        self.total_samples = self.lil_mat.sum()\n",
    "        self.batches = np.ceil(self.total_samples / self.batch_size).astype(np.int32)\n",
    "        self.last_batch_size = np.mod(self.total_samples, self.batch_size)\n",
    "        self.vocabulary_size = self.lil_mat.shape[0]\n",
    "\n",
    "        # Create a copy of the matrix to restore\n",
    "        self.lil_mat_restore = self.lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat_restore = np.copy(self.lil_mat)\n",
    "        \n",
    "        # Nonzero index matrix and index of nonzero index matrix\n",
    "        self.nonzero_indices = np.array(self.lil_mat.nonzero()).T # Transpose = zip!\n",
    "        self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "        \n",
    "    def restore_mat(self):\n",
    "        self.lil_mat = self.lil_mat_restore.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(self.lil_mat_restore)\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "\n",
    "        # If we already drew all samples, restore everything and restart\n",
    "        if self.nonzero_indices_indices.shape[0] == 0:\n",
    "            self.restore_mat()\n",
    "            self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "\n",
    "        # Get batch_size samples, or whatever remains\n",
    "        if self.batch_size > self.nonzero_indices_indices.shape[0]:\n",
    "            sample_size = self.nonzero_indices_indices.shape[0]\n",
    "        else:\n",
    "            sample_size = self.batch_size\n",
    "        negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        # Shuffeling takes place here!\n",
    "        draw = np.random.choice(self.nonzero_indices_indices, size=sample_size, replace=False)\n",
    "        ii = self.nonzero_indices[draw].T[0]\n",
    "        pos_jj = self.nonzero_indices[draw].T[1]\n",
    "        \n",
    "        # Subsampling\n",
    "        if sampling_factor > 0:\n",
    "            rr = np.random.random(size=ii.shape[0])\n",
    "            mm = np.array(sampling_table[ii+1] > rr) # Creates a boolean mask of True where i>r\n",
    "            ii = ii[mm] # Where the boolean mask is true, an entry will be kept\n",
    "            pos_jj=pos_jj[mm]\n",
    "            sample_size = ii.shape[0] # Adjust sample size\n",
    "            negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        words = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        words = np.tile(ii, 1 + self.negative_samples)\n",
    "        contexts = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        labels = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "\n",
    "        # Positive contexts and labels\n",
    "        ones = np.ones(sample_size, dtype=np.int32)\n",
    "        contexts[0:sample_size] = pos_jj\n",
    "        labels[0:sample_size] = ones\n",
    "\n",
    "        # Negative contexts and labels\n",
    "        neg_jj = np.random.randint(self.vocabulary_size, size=negative_sample_size)\n",
    "        zeros = np.zeros(negative_sample_size)\n",
    "        contexts[sample_size:] = neg_jj\n",
    "        labels[sample_size:] = zeros\n",
    "\n",
    "        # Deduct 1 from every positive sample in the matrix\n",
    "        samples = self.lil_mat[ii, pos_jj].toarray()\n",
    "        # samples = self.lil_mat[ii, pos_jj]\n",
    "        samples-=1\n",
    "\n",
    "        # \"Delete\" sample by deleting (=boolean masking) the index to its index\n",
    "        delete = np.where(samples == 0)[0]\n",
    "        self.nonzero_indices_indices = np.delete(self.nonzero_indices_indices, delete)\n",
    "        \n",
    "        # \"Keep\" sample by just reducing its value in the matrix\n",
    "        keep = np.where(samples > 0)[0]\n",
    "        for k in keep:\n",
    "            self.lil_mat[ii[k], pos_jj[k]]-=1\n",
    "        # self.lil_mat[ii[keep], pos_jj[keep]]-=1 # Not working with LIL\n",
    "\n",
    "        # Prepare data for training\n",
    "        x = [words, contexts]\n",
    "        return x, labels\n",
    "         \n",
    "    def __len__(self):\n",
    "        return self.batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.get_batch(idx)\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "15261/15261 [==============================] - 9239s 605ms/step - loss: 0.3762 - acc: 0.8386\n",
      "['queen'] [] [('elizabeth', 0.4707738161087036), ('king', 0.4355047941207886), ('crown', 0.400992214679718)]\n",
      "\n",
      "Epoch 00001: saving model to weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29144ac278>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We already shuffle within the generator\n",
    "data = WordContextMatrixDataset(lil_mat, positive_samples, negative_samples)\n",
    "tester = TestCallback(testword='queen')\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1)\n",
    "sgns.fit_generator(data, epochs=epochs, shuffle=False, callbacks=[tester, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'king'] ['man'] [('queen', 0.3450177311897278), ('marry', 0.31643739342689514), ('princess', 0.3108462393283844), ('napoleon', 0.3016669750213623), ('elizabeth', 0.29752159118652344), ('mary', 0.29609185457229614), ('oath', 0.28831785917282104), ('shelley', 0.2847101092338562), ('lady', 0.2829630970954895), ('sultan', 0.2817012667655945)]\n"
     ]
    }
   ],
   "source": [
    "tester.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
