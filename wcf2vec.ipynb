{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original paper: https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "\n",
    "C implementation commentary: https://github.com/chrisjmccormick/word2vec_commented\n",
    "\n",
    "Python only implementation (without negative sampling): https://github.com/cbellei/word2veclite\n",
    "\n",
    "Keras skipgram function: https://keras.io/preprocessing/sequence/#skipgrams\n",
    "\n",
    "Implementation of Keras skipgram function:https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py\n",
    "\n",
    "Full Keras example: https://github.com/Hironsan/awesome-embedding-models/blob/master/examples/skip-gram_with_ns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n",
      "Keras version: 2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import gensim\n",
    "import os\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version: ' + tf.__version__)\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import keras\n",
    "print('Keras version: ' + keras.__version__)\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: implement loading of pickled wcf matrix\n",
    "# Load from pickle file\n",
    "# with open('data.pic', 'rb') as f:\n",
    "#     source = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "corpus = 'c/proust_ascii.txt'\n",
    "dim = 300 # word2vec default: 300\n",
    "window_size = 5 # word2vec default: 5\n",
    "# min_count = 5 # word2vec default: 5 - set to 0 to disable\n",
    "max_vocab = 10000\n",
    "sampling_factor = 1e-05 # word2vec default: 1e-03 - set to 0 to disable\n",
    "positive_samples = 10000 # gensim.word2vec.Text8Corpus default: 10000 words/sequence -> Keras skipgram function\n",
    "negative_samples = 5 # word2vec default: 5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text size: 7049641 characters\n",
      "Converting text to word list...\n"
     ]
    }
   ],
   "source": [
    "# Read corpus\n",
    "with open(corpus, 'r') as f: text = f.read()\n",
    "\n",
    "# Make it all lower case    \n",
    "text = text.lower()\n",
    "\n",
    "# Remove all punctuation\n",
    "exclude = set(string.punctuation)\n",
    "text = ''.join(char for char in text if char not in exclude)\n",
    "\n",
    "print('Text size: '+ str(len(text)) + ' characters')\n",
    "\n",
    "# Truncate to a million characters for quick testing\n",
    "# text = text[:1000000]\n",
    "\n",
    "# Make list\n",
    "print('Converting text to word list...')\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum frequency: 5, word: rivalry\n",
      "Vocabulary size: 10000 words\n"
     ]
    }
   ],
   "source": [
    "# Get frequencies\n",
    "frequencies = Counter(text).most_common()\n",
    "\n",
    "# Eliminate words that appear less than min_count times\n",
    "# frequencies = OrderedDict({word: i for word, i in frequencies if i >= min_count})\n",
    "\n",
    "# Keep the vocabulary to max_vocab\n",
    "frequencies = OrderedDict(frequencies[:max_vocab])\n",
    "min_frequency = min(frequencies, key=frequencies.get)\n",
    "print('Minimum frequency: ' + str(frequencies[min_frequency]) + ', word: ' + str(min_frequency))\n",
    "\n",
    "# Map words to integers\n",
    "token2id = {word: i for i, word in enumerate(frequencies.keys())}\n",
    "id2token = {i: word for i, word in enumerate(frequencies.keys())}\n",
    "\n",
    "# Length of dictionary is size of vocabulary\n",
    "vocabulary_size = len(token2id)\n",
    "\n",
    "# Encode\n",
    "text = [token2id[token] for token in text if token in token2id]\n",
    "\n",
    "# Make sampling table for subsampling (starts at 1!)\n",
    "sampling_table = make_sampling_table(size=vocabulary_size+1, sampling_factor=sampling_factor)\n",
    "\n",
    "print('Vocabulary size: ' + str(vocabulary_size) + ' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling wcf matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741791c0407444af95ea833d4f48285b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1273823), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples in matrix: 12738200 samples\n",
      "Creating sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "# Create square word-context-frequency matrix\n",
    "# TO DO: Make this faster with multiprocessing\n",
    "print('Filling wcf matrix...')\n",
    "mat = np.zeros((vocabulary_size, vocabulary_size), dtype=np.int32)\n",
    "for i, wi in (enumerate(tqdm_notebook(text))):\n",
    "    window_start = max(0, i - window_size)\n",
    "    window_end = min(len(text), i + window_size + 1)\n",
    "    for j in range(window_start, window_end):\n",
    "        if j != i:\n",
    "            wj = text[j]\n",
    "            mat[wi][wj]+=1            \n",
    "print('Total samples in matrix: ' + str(mat.sum()) + ' samples')\n",
    "        \n",
    "# Create sparse LIL representation of square word-context matrix\n",
    "print('Creating sparse matrix...')\n",
    "coo_mat = coo_matrix(mat)\n",
    "lil_mat = coo_mat.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 300)       3000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 300)       3000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,000,000\n",
      "Trainable params: 6,000,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# SOURCE: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n",
    "\n",
    "print('Building model...')\n",
    "\n",
    "t_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "t = Embedding(vocabulary_size, dim)(t_inputs)\n",
    "c_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "c  = Embedding(vocabulary_size, dim)(c_inputs)\n",
    "o = Dot(axes=2)([t, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "sgns = Model(inputs=[t_inputs, c_inputs], outputs=o)\n",
    "sgns.summary()\n",
    "sgns.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTest(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.gensim_model = self.make_gensim_model(self.model.get_weights()[0])\n",
    "        print(self.gensim_model.most_similar(self.query, topn=3))\n",
    "        \n",
    "    def make_gensim_model(self, weights):\n",
    "        # TO DO: There must be a way to do this without I/O\n",
    "        with open('vectors.txt' ,'w') as f:\n",
    "            f.write('{} {}\\n'.format(vocabulary_size, dim))\n",
    "            for token, i in token2id.items():\n",
    "                f.write('{} {}\\n'.format(token, ' '.join(map(str, list(weights[i,:])))))\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordContextMatrixDataset(Sequence):\n",
    "\n",
    "    def __init__(self, lil_mat, positive_samples=10000, negative_samples=5):\n",
    "\n",
    "        self.lil_mat = lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(lil_mat)\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "        self.batch_size = positive_samples + negative_samples\n",
    "        self.total_samples = self.lil_mat.sum()\n",
    "        self.batches = np.ceil(self.total_samples / self.batch_size).astype(np.int32)\n",
    "        self.last_batch_size = np.mod(self.total_samples, self.batch_size)\n",
    "        self.vocabulary_size = self.lil_mat.shape[0]\n",
    "\n",
    "        # Create a copy of the matrix to restore\n",
    "        self.lil_mat_restore = self.lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat_restore = np.copy(self.lil_mat)\n",
    "        \n",
    "        # Nonzero index matrix and index of nonzero index matrix\n",
    "        self.nonzero_indices = np.array(self.lil_mat.nonzero()).T # Transpose = zip!\n",
    "        self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "        \n",
    "    def restore_mat(self):\n",
    "        self.lil_mat = self.lil_mat_restore.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(self.lil_mat_restore)\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "\n",
    "        # If we already drew all samples, restore everything and restart\n",
    "        if self.nonzero_indices_indices.shape[0] == 0:\n",
    "            self.restore_mat()\n",
    "            self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "\n",
    "        # Get batch_size samples, or whatever remains\n",
    "        if self.batch_size > self.nonzero_indices_indices.shape[0]:\n",
    "            sample_size = self.nonzero_indices_indices.shape[0]\n",
    "        else:\n",
    "            sample_size = self.batch_size\n",
    "        negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        # Shuffeling takes place here!\n",
    "        draw = np.random.choice(self.nonzero_indices_indices, size=sample_size, replace=False)\n",
    "        ii = self.nonzero_indices[draw].T[0]\n",
    "        pos_jj = self.nonzero_indices[draw].T[1]\n",
    "        \n",
    "        # Subsampling\n",
    "        if sampling_factor > 0:\n",
    "            rr = np.random.random(size=ii.shape[0])\n",
    "            mm = np.array(sampling_table[ii+1] > rr) # Creates a boolean mask of True where i>r\n",
    "            ii = ii[mm] # Where the boolean mask is true, an entry will be kept\n",
    "            pos_jj=pos_jj[mm]\n",
    "            sample_size = ii.shape[0] # Adjust sample size\n",
    "            negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        words = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        words = np.tile(ii, 1 + self.negative_samples)\n",
    "        contexts = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        labels = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "\n",
    "        # Positive contexts and labels\n",
    "        ones = np.ones(sample_size, dtype=np.int32)\n",
    "        contexts[0:sample_size] = pos_jj\n",
    "        labels[0:sample_size] = ones\n",
    "\n",
    "        # Negative contexts and labels\n",
    "        neg_jj = np.random.randint(self.vocabulary_size, size=negative_sample_size)\n",
    "        zeros = np.zeros(negative_sample_size)\n",
    "        contexts[sample_size:] = neg_jj\n",
    "        labels[sample_size:] = zeros\n",
    "\n",
    "        # Deduct 1 from every positive sample in the matrix\n",
    "        samples = self.lil_mat[ii, pos_jj].toarray()\n",
    "        # samples = self.lil_mat[ii, pos_jj]\n",
    "        samples-=1\n",
    "\n",
    "        # \"Delete\" sample by deleting (=boolean masking) the index to its index\n",
    "        delete = np.where(samples == 0)[0]\n",
    "        self.nonzero_indices_indices = np.delete(self.nonzero_indices_indices, delete)\n",
    "        \n",
    "        # \"Keep\" sample by just reducing its value in the matrix\n",
    "        keep = np.where(samples > 0)[0]\n",
    "        for k in keep:\n",
    "            self.lil_mat[ii[k], pos_jj[k]]-=1\n",
    "        # self.lil_mat[ii[keep], pos_jj[keep]]-=1 # Not working with LIL\n",
    "\n",
    "        # Prepare data for training\n",
    "        x = [words, contexts]\n",
    "        return x, labels\n",
    "         \n",
    "    def __len__(self):\n",
    "        return self.batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.get_batch(idx)\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1274/1274 [==============================] - 84s 66ms/step - loss: 0.3660 - acc: 0.8480\n",
      "[('pepsin', 0.8590530753135681), ('deplored', 0.8588818311691284), ('raining', 0.8569839000701904)]\n",
      "\n",
      "Epoch 00001: saving model to ./model.ckpt\n",
      "Epoch 2/5\n",
      "1274/1274 [==============================] - 84s 66ms/step - loss: 0.2698 - acc: 0.8948\n",
      "[('king', 0.6958807706832886), ('nephew', 0.6936745047569275), ('naples', 0.68967604637146)]\n",
      "\n",
      "Epoch 00002: saving model to ./model.ckpt\n",
      "Epoch 3/5\n",
      "1274/1274 [==============================] - 84s 66ms/step - loss: 0.2336 - acc: 0.9054\n",
      "[('naples', 0.5699007511138916), ('king', 0.5498378276824951), ('occasion', 0.526729941368103)]\n",
      "\n",
      "Epoch 00003: saving model to ./model.ckpt\n",
      "Epoch 4/5\n",
      "1274/1274 [==============================] - 84s 66ms/step - loss: 0.1980 - acc: 0.9201\n",
      "[('naples', 0.5291523337364197), ('king', 0.47386306524276733), ('france', 0.4501396715641022)]\n",
      "\n",
      "Epoch 00004: saving model to ./model.ckpt\n",
      "Epoch 5/5\n",
      "1274/1274 [==============================] - 83s 65ms/step - loss: 0.1723 - acc: 0.9318\n",
      "[('naples', 0.49518439173698425), ('duchesse', 0.4141024649143219), ('king', 0.41221535205841064)]\n",
      "\n",
      "Epoch 00005: saving model to ./model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63e328e940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We already shuffle within the generator\n",
    "data = WordContextMatrixDataset(lil_mat, positive_samples, negative_samples)\n",
    "tester = ModelTest(query='queen')\n",
    "checkpointer = ModelCheckpoint(filepath='./model.ckpt', verbose=1) # Different from the model for visualization\n",
    "sgns.fit_generator(data, epochs=epochs, shuffle=False, callbacks=[tester, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./projector/model.ckpt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create files for Tensorboard embedding projector from internal gensim model\n",
    "# NOTE: We need to do this in plain Tensorflow because Keras gets confused by the double embedding layer\n",
    "# TO DO: Open Keras issue\n",
    "\n",
    "path = './projector'\n",
    "if not os.path.exists(path): os.makedirs(path)\n",
    "\n",
    "vectors = np.zeros((max_vocab, dim))\n",
    "with open(path + '/metadata.tsv', 'w+') as f:\n",
    "    for i, word in enumerate(tester.gensim_model.wv.index2word):\n",
    "        vectors[i] = tester.gensim_model[word]\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding = tf.Variable(vectors, trainable=False, name='embedding')\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(path, sess.graph)\n",
    "config = projector.ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.tensor_name = 'embedding'\n",
    "embed.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(writer, config)\n",
    "saver.save(sess, path + '/model.ckpt')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
