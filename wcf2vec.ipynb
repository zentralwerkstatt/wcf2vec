{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings for Restricted Access Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "This version (c) 2018 Fabian Offert for the WE1S project, license: [CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Original word2vec paper: https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "\n",
    "C implementation commentary: https://github.com/chrisjmccormick/word2vec_commented\n",
    "\n",
    "Python only implementation (without negative sampling): https://github.com/cbellei/word2veclite\n",
    "\n",
    "Keras skipgram function: https://keras.io/preprocessing/sequence/#skipgrams\n",
    "\n",
    "Implementation of Keras skipgram function: https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py\n",
    "\n",
    "Full Keras example: https://github.com/Hironsan/awesome-embedding-models/blob/master/examples/skip-gram_with_ns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n",
      "Keras version: 2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import gensim\n",
    "import os\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version: ' + tf.__version__)\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import keras\n",
    "print('Keras version: ' + keras.__version__)\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "corpus = 'corpora/txtlab.txt'\n",
    "dim = 300 # word2vec default: 300\n",
    "window_size = 5 # word2vec default: 5\n",
    "# min_count = 5 # word2vec default: 5 - set to 0 to disable\n",
    "max_vocab = 10000\n",
    "sampling_factor = 1e-03 # word2vec default: 1e-03 - set to 0 to disable\n",
    "positive_samples = 10000 # gensim.word2vec.Text8Corpus default: 10000 words/sequence â†’ Keras skipgram function\n",
    "negative_samples = 5 # word2vec default: 5\n",
    "epochs = 5\n",
    "dynamic_windowing = True # word2vec default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text size: 99554089 characters\n",
      "Converting text to word list...\n"
     ]
    }
   ],
   "source": [
    "# Read corpus\n",
    "with open(corpus, 'r') as f: text = f.read()\n",
    "\n",
    "# Make it all lower case    \n",
    "text = text.lower()\n",
    "\n",
    "# Remove all punctuation\n",
    "exclude = set(string.punctuation)\n",
    "text = ''.join(char for char in text if char not in exclude)\n",
    "\n",
    "print('Text size: '+ str(len(text)) + ' characters')\n",
    "\n",
    "# Truncate to a million characters for quick testing\n",
    "# text = text[:1000000]\n",
    "\n",
    "# Make list\n",
    "print('Converting text to word list...')\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum frequency: 97, word: corrected\n",
      "Vocabulary size: 10000 words\n"
     ]
    }
   ],
   "source": [
    "# Get frequencies\n",
    "frequencies = Counter(text).most_common()\n",
    "\n",
    "# Eliminate words that appear less than min_count times\n",
    "# frequencies = OrderedDict({word: i for word, i in frequencies if i >= min_count})\n",
    "\n",
    "# Keep the vocabulary to max_vocab\n",
    "frequencies = OrderedDict(frequencies[:max_vocab])\n",
    "min_frequency = min(frequencies, key=frequencies.get)\n",
    "print('Minimum frequency: ' + str(frequencies[min_frequency]) + ', word: ' + str(min_frequency))\n",
    "\n",
    "# Map words to integers\n",
    "token2id = {word: i for i, word in enumerate(frequencies.keys())}\n",
    "id2token = {i: word for i, word in enumerate(frequencies.keys())}\n",
    "\n",
    "# Length of dictionary is size of vocabulary\n",
    "vocabulary_size = len(token2id)\n",
    "\n",
    "# Encode\n",
    "text = [token2id[w] for w in text if w in token2id]\n",
    "\n",
    "# Make sampling table for subsampling (starts at 1!)\n",
    "sampling_table = make_sampling_table(size=vocabulary_size+1, sampling_factor=sampling_factor)\n",
    "\n",
    "print('Vocabulary size: ' + str(vocabulary_size) + ' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling wcf matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a21cb9d00c4f9da0523231b0c36de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17336804), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples in matrix: 79171394.93333432 samples\n",
      "Creating sparse integer matrix...\n",
      "74261431\n"
     ]
    }
   ],
   "source": [
    "# Create square word-context-frequency matrix\n",
    "print('Filling wcf matrix...')\n",
    "mat = np.zeros((vocabulary_size, vocabulary_size))\n",
    "for i, wi in (enumerate(tqdm_notebook(text))):\n",
    "    window_start = max(0, i - window_size)\n",
    "    window_end = min(len(text), i + window_size + 1)\n",
    "    for j in range(window_start, window_end):\n",
    "        if j != i:\n",
    "            wj = text[j]\n",
    "            if dynamic_windowing: mat[wi][wj]+=(1/abs(i-j))\n",
    "            else: mat[wi][wj]+=1            \n",
    "print('Total samples in matrix: ' + str(mat.sum()) + ' samples')\n",
    "        \n",
    "# Create sparse LIL representation of square word-context matrix\n",
    "print('Creating sparse integer matrix...')\n",
    "coo_mat = coo_matrix(mat.astype(np.int32), copy=True)\n",
    "del mat # Reclaim memory\n",
    "lil_mat = coo_mat.tolil()\n",
    "print(lil_mat.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 300)       3000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 300)       3000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,000,000\n",
      "Trainable params: 6,000,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# SOURCE: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n",
    "\n",
    "print('Building model...')\n",
    "\n",
    "t_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "t = Embedding(vocabulary_size, dim)(t_inputs)\n",
    "c_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "c  = Embedding(vocabulary_size, dim)(c_inputs)\n",
    "o = Dot(axes=2)([t, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "sgns = Model(inputs=[t_inputs, c_inputs], outputs=o)\n",
    "sgns.summary()\n",
    "sgns.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTest(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.gensim_model = self.make_gensim_model(self.model.get_weights()[0])\n",
    "        print(self.gensim_model.most_similar(self.query, topn=3))\n",
    "        \n",
    "    def make_gensim_model(self, weights):\n",
    "        # TO DO: There must be a way to do this without I/O\n",
    "        with open('vectors.txt' ,'w') as f:\n",
    "            f.write('{} {}\\n'.format(vocabulary_size, dim))\n",
    "            for token, i in token2id.items():\n",
    "                f.write('{} {}\\n'.format(token, ' '.join(map(str, list(weights[i,:])))))\n",
    "        return gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordContextMatrixDataset(Sequence):\n",
    "\n",
    "    def __init__(self, lil_mat, positive_samples=10000, negative_samples=5):\n",
    "\n",
    "        self.lil_mat = lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(lil_mat)\n",
    "        self.positive_samples = positive_samples\n",
    "        self.negative_samples = negative_samples\n",
    "        self.batch_size = positive_samples + negative_samples\n",
    "        self.total_samples = self.lil_mat.sum()\n",
    "        self.batches = np.ceil(self.total_samples / self.batch_size).astype(np.int32)\n",
    "        self.last_batch_size = np.mod(self.total_samples, self.batch_size)\n",
    "        self.vocabulary_size = self.lil_mat.shape[0]\n",
    "\n",
    "        # Create a copy of the matrix to restore\n",
    "        self.lil_mat_restore = self.lil_mat.tolil(copy=True)\n",
    "        # self.lil_mat_restore = np.copy(self.lil_mat)\n",
    "        \n",
    "        # Nonzero index matrix and index of nonzero index matrix\n",
    "        self.nonzero_indices = np.array(self.lil_mat.nonzero()).T # Transpose = zip!\n",
    "        self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "        \n",
    "    def restore_mat(self):\n",
    "        self.lil_mat = self.lil_mat_restore.tolil(copy=True)\n",
    "        # self.lil_mat = np.copy(self.lil_mat_restore)\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "\n",
    "        # If we already drew all samples, restore everything and restart\n",
    "        if self.nonzero_indices_indices.shape[0] == 0:\n",
    "            self.restore_mat()\n",
    "            self.nonzero_indices_indices = np.array(np.arange(self.nonzero_indices.shape[0]))\n",
    "\n",
    "        # Get batch_size samples, or whatever remains\n",
    "        if self.batch_size > self.nonzero_indices_indices.shape[0]:\n",
    "            sample_size = self.nonzero_indices_indices.shape[0]\n",
    "        else:\n",
    "            sample_size = self.batch_size\n",
    "        negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        # Shuffeling takes place here!\n",
    "        draw = np.random.choice(self.nonzero_indices_indices, size=sample_size, replace=False)\n",
    "        ii = self.nonzero_indices[draw].T[0]\n",
    "        pos_jj = self.nonzero_indices[draw].T[1]\n",
    "        \n",
    "        # Subsampling\n",
    "        if sampling_factor > 0:\n",
    "            rr = np.random.random(size=ii.shape[0])\n",
    "            mm = np.array(sampling_table[ii+1] > rr) # Creates a boolean mask of True where i>r\n",
    "            ii = ii[mm] # Where the boolean mask is true, an entry will be kept\n",
    "            pos_jj=pos_jj[mm]\n",
    "            sample_size = ii.shape[0] # Adjust sample size\n",
    "            negative_sample_size = sample_size * self.negative_samples\n",
    "\n",
    "        words = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        words = np.tile(ii, 1 + self.negative_samples)\n",
    "        contexts = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "        labels = np.zeros(sample_size + negative_sample_size, dtype=np.int32)\n",
    "\n",
    "        # Positive contexts and labels\n",
    "        ones = np.ones(sample_size, dtype=np.int32)\n",
    "        contexts[0:sample_size] = pos_jj\n",
    "        labels[0:sample_size] = ones\n",
    "\n",
    "        # Negative contexts and labels\n",
    "        neg_jj = np.random.randint(self.vocabulary_size, size=negative_sample_size)\n",
    "        zeros = np.zeros(negative_sample_size)\n",
    "        contexts[sample_size:] = neg_jj\n",
    "        labels[sample_size:] = zeros\n",
    "\n",
    "        # Deduct 1 from every positive sample in the matrix\n",
    "        samples = self.lil_mat[ii, pos_jj].toarray()\n",
    "        # samples = self.lil_mat[ii, pos_jj]\n",
    "        samples-=1\n",
    "\n",
    "        # \"Delete\" sample by deleting (=boolean masking) the index to its index\n",
    "        delete = np.where(samples == 0)[0]\n",
    "        self.nonzero_indices_indices = np.delete(self.nonzero_indices_indices, delete)\n",
    "        \n",
    "        # \"Keep\" sample by just reducing its value in the matrix\n",
    "        keep = np.where(samples > 0)[0]\n",
    "        for k in keep:\n",
    "            self.lil_mat[ii[k], pos_jj[k]]-=1\n",
    "        # self.lil_mat[ii[keep], pos_jj[keep]]-=1 # Not working with LIL\n",
    "\n",
    "        # Prepare data for training\n",
    "        x = [words, contexts]\n",
    "        return x, labels\n",
    "         \n",
    "    def __len__(self):\n",
    "        return self.batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.get_batch(idx)\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7423/7423 [==============================] - 1468s 198ms/step - loss: 0.3048 - acc: 0.8696\n",
      "[('helene', 0.43006783723831177), ('princess', 0.41735637187957764), ('sovereign', 0.40093570947647095)]\n",
      "Epoch 2/5\n",
      "7423/7423 [==============================] - 1466s 197ms/step - loss: 0.2395 - acc: 0.8886\n",
      "[('princess', 0.32642680406570435), ('march', 0.3052293658256531), ('ministry', 0.2952641248703003)]\n",
      "Epoch 3/5\n",
      "7423/7423 [==============================] - 1463s 197ms/step - loss: 0.2220 - acc: 0.8951\n",
      "[('princess', 0.31089067459106445), ('ministry', 0.2957751154899597), ('priest', 0.28572994470596313)]\n",
      "Epoch 4/5\n",
      "7423/7423 [==============================] - 1466s 197ms/step - loss: 0.2149 - acc: 0.8980\n",
      "[('princess', 0.3147178888320923), ('victoria', 0.2861301898956299), ('wench', 0.28555524349212646)]\n",
      "Epoch 5/5\n",
      "7423/7423 [==============================] - 1461s 197ms/step - loss: 0.2109 - acc: 0.8996\n",
      "[('princess', 0.31343764066696167), ('victoria', 0.30550509691238403), ('ministry', 0.2916772663593292)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2d728ea20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We already shuffle within the generator\n",
    "data = WordContextMatrixDataset(lil_mat, positive_samples, negative_samples)\n",
    "tester = ModelTest(query='queen')\n",
    "checkpointer = ModelCheckpoint(filepath='model.ckpt') # Different from the model for visualization\n",
    "sgns.fit_generator(data, epochs=epochs, shuffle=False, callbacks=[tester, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prince', 0.2706182897090912), ('servant', 0.24704021215438843), ('sister', 0.24513643980026245), ('madam', 0.22692053020000458), ('wife', 0.22593748569488525), ('nurse', 0.22547906637191772), ('anne', 0.22380417585372925), ('maiden', 0.2230634242296219), ('briggs', 0.21479201316833496), ('girl', 0.2124357521533966)]\n"
     ]
    }
   ],
   "source": [
    "print(tester.gensim_model.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./projector/model.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create files for Tensorboard embedding projector from internal gensim model\n",
    "path = 'projector'\n",
    "if not os.path.exists(path): os.makedirs(path)\n",
    "\n",
    "vectors = np.zeros((max_vocab, dim))\n",
    "with open(path + '/metadata.tsv', 'w+') as f:\n",
    "    for i, word in enumerate(tester.gensim_model.wv.index2word):\n",
    "        vectors[i] = tester.gensim_model[word]\n",
    "        f.write(word + '\\n')\n",
    "        \n",
    "# Plain Tensorflow because Keras gets confused by the double embedding layer\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding = tf.Variable(vectors, trainable=False, name='embedding')\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(path, sess.graph)\n",
    "config = projector.ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.tensor_name = 'embedding'\n",
    "embed.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(writer, config)\n",
    "saver.save(sess, path + '/model.ckpt')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
